TESTING METHODOLOGY

1. What's the right role for QA in the software development process?

Testing should involve a very highly repeated testing cycle.  One that is exercised constantly and iteratively to expose changes that result in defects compared to previous behavior.  There are so many different QA roles in the software development process that it's hard to say there is a specific right one.  Unit testing, Integration Testing, Performance Testing, Destructive Testing, Stress Testing, Browser Testing, Database Testing, API Testing, Manual Testing, Infrastructure & Network Testing.  When it's possible to automate a large portion of these various testing types into the build pipeline with transparency in test coverage, the delivery pipeline can reveal defects much more quickly.

2. As a developer focused on quality, you have 2 weeks to prepare before your team starts writing software.  What do you do?

Attend all design and architecture discussions.  Pinpoint integrations, schema definitions and try to document APIs and protocols.  Sort out Authorization if there is any.  Sort out configuration and deployment details, environment specifics and any other infrastructure related requirements.  Based on the designs and any other known information, draft a test plan, test suites, test cases, etc.  Make determinations about tools used for any automation, load, stress or performance testing and how to measure test metrics.

3. When is it appropriate to use automated testing?  When is it appropriate to use manual testing?

Automated testing is best suited to tasks that are repetitive.  It is easy to feed automated tests with data, so data driven tests with lots of data permutations lend themselves well to automated testing whereas testing all those conditions manually would be very time consuming and more error prone.  Automated testing is also a great choice for any mission critical functionality that needs to be smoke tested.  A quick sanity test that all the most important functionality is working.  It can be useful for browser testing in the cloud, as repeating the same tests manually across a large matrix of browser and OS combinations can be prohibitively time consuming manually.  Automation is good for API testing and any kind of mock object testing that needs to be done to emulate test conditions that don't exist against 3rd parties in a test scenario.  There are many, many applications for automated testing.

Manual Testing is good for things that automated testing isn't very good at.  Image rendering, visual display issues and sometimes JavaScript issues in the browser.  These are all issues that are more difficult to automate and yet quite easy to spot with Manual testing.  Picking random paths through the application or trying unique combinations of user actions in the UI is another thing automation is bad at.  General look and feel of the UI for new features, integrations and implementations is something automation can't really test and for which Manual Testing is required.

4. Your dev team has just modified an existing product by adding new features and refactoring the code for old features. The devs claim to have written unit tests; you're in charge of integration testing. Dedicated teams are handling performance and security testing, so you don't have to. As is always the case in the real world, you don't have time to test everything. What factors do you think about as you decide where to focus your testing efforts? How do you decide what not to test?

Rerun all regression tests that exist for any integration cases.  Any functionality tests that span the integrations need to be run.  Run any new test cases for the new features, manual or otherwise.  If there are any new integrations that have been introduced, test those specifically and add them to the automation suite.  Run the unit tests that the devs claim have been written and ensure they pass.  Evaluate test coverage of those unit tests in Cobertura or some other similar code coverage tool to ensure the code is being tested adequately and passing.  Look through the variety of test paths and prioritize the test cases that will be run based on severity of issue if they fail and commonality of usage of the test path.  Any path where a software defect creates severe impairment or the defect is on a test path that is very commonly used should be tested first.  Test execution should be prioritized starting here and working backwards.